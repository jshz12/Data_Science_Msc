{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AE0X6qZTlqshyxNzNqD4gmLTrEmXMuQj","timestamp":1714327594955}],"gpuType":"T4","collapsed_sections":["WiJT5408ee_X","iwUZ1wdGd78A"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import numpy as np\n","import gensim.downloader as api #word2vec\n","from scipy.spatial.distance import cosine,euclidean,cityblock #distances"],"metadata":{"id":"EMcqfKi-dlor"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Embeddings"],"metadata":{"id":"WiJT5408ee_X"}},{"cell_type":"markdown","source":["Here we can see two different pre-trained models to compute embeddings.\n","The first one loads the pre-trained Google News Word2Vec model with 300-dimensional vectors while the second model loads the pre-trained Facebook model with, again, 300-dimensional vectors.\n","\n","In both functions the input will be the text (the question) for which we want to create the emedding and as an output we will obtain the mean of embeddings of all the words in the text."],"metadata":{"id":"kCfZUZd1nBIl"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","w2vec_model = api.load(\"word2vec-google-news-300\" )\n","def word2vec_embeddings(text,model):\n","\n","    # Lowercase and split it into individual words\n","    words = text.lower().split()\n","\n","    # Get embeddings for all words that are present in the model's vocabulary\n","    embeddings = [model.get_vector(word) for word in words if word in model.index_to_key]\n","\n","    # If no embeddings were found, return a null embedding\n","    if len(embeddings) == 0:\n","        return None\n","    else:\n","        # Take the mean of all embeddings to get a single embedding for the entire text\n","        return np.mean(embeddings, axis=0)"],"metadata":{"id":"uUX99CEdeQSU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fasttext\n","import fasttext.util\n","fasttext.util.download_model('en', if_exists='ignore')\n","ft_model = fasttext.load_model('cc.en.300.bin')\n","\n","def fasttext_embeddings(text,model):\n","\n","    # Lowercase and split it into individual words\n","    words = text.lower().split()\n","\n","    # Get embeddings for all words that are present in the model's vocabulary\n","    embeddings = [model.get_word_vector(word) for word in words if word in model]\n","    # If no embeddings were found, return None\n","    if len(embeddings) == 0:\n","        return None\n","    else:\n","        # Take the mean of all embeddings to get a single embedding for the entire text\n","        return np.mean(embeddings, axis=0)"],"metadata":{"id":"RtAEsvmwm9rc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We realized that the second model was way faster than the first one when computing the embeddings"],"metadata":{"id":"LdpUNV3bv0rq"}},{"cell_type":"markdown","source":["# Distances"],"metadata":{"id":"iwUZ1wdGd78A"}},{"cell_type":"markdown","source":["The structure of the following functions will be similar. We want to measure the distance between two embeddings. Both functions will have the two embeddings (previously calculated) as an input and will return the distance (substracting to one if necessary) as an output. If either of the embeddings is None, it will return None."],"metadata":{"id":"WeMBcj-ud6Qk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sm8pcAQA71NE"},"outputs":[],"source":["#Cosine Distance\n","\n","def cos_distance(embedding1, embedding2):\n","\n","    if embedding1 is None or embedding2 is None:\n","        return None\n","    else:\n","        # Compute the cosine distance between the two embeddings\n","        return cosine(embedding1, embedding2) #we want the distance, if we set 1 - cosine(embedding1, embedding2), for two identical questions we will obtain 1 as an output\n","\n","#Euclidean Distance\n","\n","def euclidean_distance(embedding1, embedding2):\n","\n","    if embedding1 is None or embedding2 is None:\n","        return None\n","    else:\n","        # Compute the Euclidean distance between the two embeddings\n","        return euclidean(embedding1,embedding2)\n","\n","def manhattan_distance(embedding1, embedding2):\n","\n","    if embedding1 is None or embedding2 is None:\n","        return None\n","    else:\n","        # Compute the Manhattan distance between the two embeddings\n","        return cityblock(embedding1,embedding2)"]},{"cell_type":"markdown","source":["This function computes the distance between embeddings according to the previous defined distances."],"metadata":{"id":"geC6oqIVTxYo"}},{"cell_type":"code","source":["def distance_embeddings(embedding1, embedding2, index):\n","    embedding1 = np.array(embedding1)\n","    embedding2 = np.array(embedding2)\n","    if embedding1.shape != (100,) or embedding2.shape != (100,):\n","        print(\"Row:\", index)\n","        print(\"embedding1 shape:\", embedding1.shape)\n","        print(\"embedding2 shape:\", embedding2.shape)\n","\n","    # Compute distances between corresponding elements in the embeddings\n","    distances_cos = cosine(embedding1, embedding2)\n","    distances_euc = euclidean(embedding1, embedding2)\n","    distances_manh = cityblock(embedding1, embedding2)\n","\n","    return distances_cos, distances_euc, distances_manh"],"metadata":{"id":"Gjf_dfqxfKKE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final Approach"],"metadata":{"id":"QuNTSjDra9mQ"}},{"cell_type":"markdown","source":["We realized that Fast Text took a considerable time to calculate the embeddings. Therefore, we chose not to use a pre-trained model and train the fasttext ourselves based on the questions we had available. This final approach will have two variants: the supervised and the unsupervised way."],"metadata":{"id":"MdGlw6aJbEXL"}},{"cell_type":"markdown","source":["## Supervised"],"metadata":{"id":"V2yg-uV0eXBE"}},{"cell_type":"markdown","source":["In order to train the fasttext model we need to prepare a certain format for the questions, that is:\n","\n","__ label __positive questions...\n","\n","__ label __negative questions...\n","\n","\n","In our case, our labels are 0 (different questions) or 1 (similar questions)."],"metadata":{"id":"MwGoYhbkhMY9"}},{"cell_type":"code","source":["def training_format_sup(x,y,char):\n","  with open(char, 'w') as f:\n","    for i, question_pair in enumerate(x):\n","        label_value = y[i]\n","        f.write(\"__label__{} {} {}\\n\".format(label_value, question_pair[0], question_pair[1]))"],"metadata":{"id":"Tu3jRP3MbjBb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, to use the function .predict (to get our results) we need to have the same format than before but without the labels."],"metadata":{"id":"mgrfRsMVizpL"}},{"cell_type":"code","source":["def questions_fasttext_format(questions):\n","  combined_questions = []\n","\n","  for pregunta_pair in questions:\n","    aux = pregunta_pair[0] + ' ' + pregunta_pair[1]\n","    combined_questions.append(aux.replace('\\n', ''))\n","\n","  return(combined_questions)"],"metadata":{"id":"IggrKOwRb-g_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just computing the predictions and the probabilities (for computing the ROC metric) for each question in our test dataset, using the trained model."],"metadata":{"id":"04FehLKujksz"}},{"cell_type":"code","source":["def predict_fasttext(questions,loaded_model):\n","  predicted_label = []\n","  predicted_prob= []\n","  for i in range(len(questions)):\n","    aux_label, aux_prob = loaded_model.predict(questions[i])\n","    predicted_label.append(int(aux_label[0][-1]))\n","    if (predicted_label[i] == 0):\n","      predicted_prob.append(1-float(aux_prob[0]))\n","    else:\n","      predicted_prob.append(float(aux_prob[0]))\n","\n","  return(predicted_label, predicted_prob)"],"metadata":{"id":"KcCeH-pleICG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A proper function to represent our results in a proper way."],"metadata":{"id":"v12ZaB3IUNX_"}},{"cell_type":"code","source":["def get_results(preds,probs, label):\n","  _ = find_threshold(probs,label)  #this is a function from David Íñiguez.\n","  cm = confusion_matrix(label,preds)\n","\n","  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n","\n","  disp.plot(cmap=plt.cm.Blues)\n","  plt.title('Confusion Matrix')\n","  plt.show()\n","\n","  print (classification_report(label, preds))\n","  accuracy = accuracy_score(label,preds)\n","  f1 = f1_score(label,preds, average='weighted')\n","  precision = precision_score(label,preds, average='weighted')\n","  recall = recall_score(label,preds, average='weighted')\n","\n","  results = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-score': f1}\n","\n","  return(results)"],"metadata":{"id":"O1FN-agjx65R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Unsupervised"],"metadata":{"id":"_RIiT1jseZfo"}},{"cell_type":"markdown","source":["In this last section we will combine the feature extractions (Alejandro Vara) obtained from the questions and the embeddings obtained from Fast Text."],"metadata":{"id":"BX-p6j-0Wz6p"}},{"cell_type":"markdown","source":["This is a function analogous to *training_format_sup* but now we dont need the label anymore."],"metadata":{"id":"PnKSru0PghUA"}},{"cell_type":"code","source":["#def training_format_unsup(x, char):\n","def format_data(questions, output_file):\n","    with open(output_file, 'w') as f:\n","        for question in questions:\n","            f.write(\"{}\\n\".format(question))"],"metadata":{"id":"V1H3JT4JJYab"},"execution_count":null,"outputs":[]}]}