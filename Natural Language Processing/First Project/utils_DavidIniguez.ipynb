{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYlxmNeJX0BI","executionInfo":{"status":"ok","timestamp":1713798945431,"user_tz":-120,"elapsed":79690,"user":{"displayName":"David Íñiguez Gómez","userId":"02398035063998638046"}},"outputId":"f1ec5c96-cf75-42da-f003-f7c3e0a8b94e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence-transformers\n","  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch import nn, optim\n","from sentence_transformers import SentenceTransformer, InputExample, losses, util\n","from sentence_transformers.evaluation import BinaryClassificationEvaluator\n","from sklearn.metrics.pairwise import cosine_similarity\n","from pprint import pprint\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score, classification_report"],"metadata":{"id":"5Ono8lCGPIqL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we define a function that fits the selected model to the training dataset, and it is monitored by the validation dataset.\n","\n","For the training loss, we will use the Contrastive Loss function. This function is defined as follows, where $D = D(X_1,X_2)$ is the distance metric:\n","\n","$L (Y, X_1, X_2) = (1-Y)\\frac{1}{2}D^2 + Y\\frac{1}{2}\\{max(0,m-D)\\}^2$\n","\n","Where Y is the label (1 if the sentences are duplicated and 0 if not). So we have 2 cases:\n","- $Y=0$. Here we have $L (Y=0, X_1, X_2) = \\frac{1}{2}D^2$, which means that the distance is going to be reduced.\n","- $Y=1$. In this case $L (Y=1, X_1, X_2) = \\frac{1}{2}\\{max(0,m-D)\\}^2$, so that the distance will increase if the distance between the vectors is greater than the margin.\n","\n","To assess the similarity between two vectors, we need an appropriate metric for distance. In two or three dimensions, Euclidean distance, which is the straight-line or \"ordinary\" distance, is often suitable for measuring the distance between two points. However, in spaces with many dimensions, Euclidean distance can be misleading as points generally appear much farther apart. In such higher-dimensional spaces, the angle between vectors becomes a more useful measure of similarity. Cosine distance, which evaluates the cosine of the angle between two vectors, is used for this purpose. The cosine value is 1 for identical vectors, 0 for orthogonal vectors, and -1 for diametrically opposed vectors. Higher cosine values indicate greater similarity. To compute cosine distance, one typically calculates the dot product of the vectors. If the vectors are not unit vectors, normalization of each vector or division by the product of their magnitudes is necessary. Note that $D$ in the contrastive loss formula has to satisfy that $D(X,X)=0$, and the cosine distance does not satisfy it, since $Cosine(X,X)=1$. To solve this, we use the Siamese distance metric, that is defined as $Siamese=1-Cosine$.\n","\n","We also include a variation of the Contrastive Loss, that is the Online Contrastive Loss. This metric selects hard positive (positives that are far apart) and hard negative pairs (negatives that are close) and computes the loss only for these pairs. This loss often yields better performances than Contrastive Loss.\n","\n","Another argument of the ContrastiveLoss is the parameter $m$ of the equation of the loss, that is the margin. It is to be noted that the representations of dissimilar pairs will only contribute to the loss if the estimated distance $D(X_1,X_2) < m$. Meaning that it will no longer care how far the negative pairs $X_1$ and $X_2$ are once this limit reaches. So, it can focus more on the difficult to embed points.\n","\n","We also define an evaluator that monitors the loss. We use here a binary classificator evaluator. This evaluator evaluates a model based on the similarity of the embeddings by calculating the accuracy of identifying similar and dissimilar sentences. The metrics are the cosine similarity as well as euclidean and Manhattan distance. The returned score is the accuracy with a specified metric.\n","\n"],"metadata":{"id":"IhTVjAYePMTd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sm8pcAQA71NE"},"outputs":[],"source":["def fit (model, df_train, df_val, loss='ContrastiveLoss', out_model=None, margin = 0.5, batch_size = 128, epochs = 8):\n","  train_examples = [InputExample(texts=[df_train[\"question1\"][i], df_train[\"question2\"][i]], label=float (df_train[\"is_duplicate\"][i])) for i in df_train.index]\n","  val_examples = [InputExample(texts=[df_val['question1'][i], df_val['question2'][i]], label=float(df_val['is_duplicate'][i])) for i in df_val.index]\n","\n","  train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=128, num_workers=2, pin_memory=True)\n","\n","  distance_metric = losses.SiameseDistanceMetric.COSINE_DISTANCE\n","\n","  if loss == 'OnlineContrastiveLoss':\n","    train_loss = losses.OnlineContrastiveLoss(model=model, distance_metric=distance_metric, margin=margin)\n","\n","  else:\n","    train_loss = losses.ContrastiveLoss(model=model, distance_metric=distance_metric, margin=margin)\n","  '''\n","  Contrastive loss Expects as input two texts and a label of either 0 or 1.\n","  If the label == 1, then the distance between the two embeddings is reduced.\n","  If the label == 0, then the distance between the embeddings is increased.\n","  Uses siamese distance metric (1- cosine).\n","  '''\n","  evaluator = BinaryClassificationEvaluator.from_input_examples(val_examples, show_progress_bar = True, batch_size=batch_size)\n","\n","  model.fit(train_objectives=[(train_dataloader, train_loss)],\n","            epochs=epochs,\n","            warmup_steps=100,\n","            evaluator=evaluator,\n","            evaluation_steps=500,\n","            save_best_model = True,\n","            output_path = os.path.join(home_dir, out_model))"]},{"cell_type":"markdown","source":["Now we define a function to find the optimal threshold that maximizes the accuracy for a given set of predictions. To do this, since we are dealing with a classification problem, we take a look at the roc curve. The threshold that maximizes the accuracy defines the operating point, the one that is closer tothe (0,1) point, where all predictions are correct. We compute the euclidean distance of the different points of the roc curve to the (0,1) point and determine which is closer to it. If wanted, the area under curve (AUC) and plot of ROC curve are shown. The function returns the threshold."],"metadata":{"id":"pkHKxiPyPR5P"}},{"cell_type":"code","source":["def find_threshold (preds, label, plot_roc=True):\n","  '''\n","  To find the optimal threshold using roc curve, so that the euclidean distance to the operating point is minimal\n","  '''\n","  # Compute ROC curve\n","  fpr, tpr, thresholds = roc_curve(label, preds)\n","\n","  # Vectorized calculation of Euclidean distance from perfect classifier point (0,1)\n","  distance_perfection = np.sqrt(fpr**2 + (1 - tpr)**2)\n","\n","  # Find index of minimum distance\n","  min_index = np.argmin(distance_perfection)\n","\n","  # Select corresponding threshold\n","  threshold = thresholds[min_index]\n","\n","  print ('Threshold: ', threshold)\n","\n","  if plot_roc:\n","    print ('AUC: %lf' % (auc(fpr, tpr)))\n","    plt.figure()\n","    plt.scatter(fpr, tpr, c= thresholds, cmap='viridis', vmin=0, vmax=1)\n","    clb = plt.colorbar()\n","    clb.ax.set_title('Threshold')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label= 'Random classifier')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC)')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","  return threshold"],"metadata":{"id":"aADU3bnxPXCy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lastly, we define a prediction function, that given a model and a DataFrame, predicts the results and prints the confusion matrix, as well as a classification report and different measures, like accuracy, precision, recall and f1 score.\n","\n","To do that, we encode the given sentences using the trained model, obtaining a distance that are going to be compared compared. In this case, we use the cosine distance metric. This result is compared with a threshold. If we provide a value for the threshold, we set to 1 those distances that are greater than the threshold, and 0 if not. If a threshold is not provided, then we determine it with the function find_threshold. We return both predictions and threshold because they could be used later, and some statistical variables that will be used later to compare different model performances.\n","\n","The user can choose if he wants to see the roc curve (remember that it can only be shown when the threshold is not provided, if not the function find_threshold is not called) and the confusion matrix and classification report."],"metadata":{"id":"ms2284WqPZ2A"}},{"cell_type":"code","source":["def predict (model, df, debug=False, threshold=None, show_roc=False, print_cm=True):\n","\n","  '''\n","  Prediction of a dataframe using a threshold. If not provided, we determine it with the function above. Returns the threshold\n","  '''\n","\n","  sentence1 = [x for x in df[\"question1\"]]\n","  sentence2 = [x for x in df[\"question2\"]]\n","\n","  if debug:\n","    print ('Encoding sentence1')\n","\n","  sentence1_embeddings = model.encode(sentence1)\n","\n","  if debug:\n","    print ('Encoding sentence2')\n","  sentence2_embeddings = model.encode(sentence2)\n","\n","  if debug:\n","    print ('Calculating distances')\n","\n","  dist = [cosine_similarity(sentence1_embeddings[i].reshape(1,-1), sentence2_embeddings[i].reshape(1,-1))[0][0] for i in range (len(df))]\n","\n","  preds = (dist - min(dist))/(max(dist) - min(dist))\n","\n","  if threshold == None:\n","    threshold = find_threshold (preds, df['is_duplicate'], plot_roc=show_roc)\n","\n","  predictions = [0 if x <= threshold else 1 for x in preds]\n","\n","  if print_cm:\n","\n","    cm = confusion_matrix(df['is_duplicate'],predictions)\n","\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n","\n","    disp.plot(cmap=plt.cm.Blues)\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","    print (classification_report(df['is_duplicate'], preds))\n","\n","  accuracy = accuracy_score(df['is_duplicate'],predictions)\n","  f1 = f1_score(df['is_duplicate'],predictions, average='weighted')\n","  precision = precision_score(df['is_duplicate'],predictions, average='weighted')\n","  recall = recall_score(df['is_duplicate'],predictions, average='weighted')\n","\n","  results = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-score': f1}\n","\n","  return predictions, threshold, results"],"metadata":{"id":"raAobz9uPesH"},"execution_count":null,"outputs":[]}]}