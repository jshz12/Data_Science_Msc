{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31682f9a-4945-4feb-bc89-83340720aa3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "#### First Name: Jaime Leonardo\n",
    "#### Last Name: Sánchez Salazar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a93105-2c23-4b4e-9362-e9c2aea7f293",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Load Data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8dbea9-daf7-4134-b06d-9c414621621b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make sure to upload the corona_tweet_new.json file\n",
    "df_twitter = spark.read.json(\"/FileStore/tables/corona_tweet_new.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32afcf7e-c82b-4c27-91e3-028eb7f11302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- created_at: string (nullable = true)\n |-- favorite_count: long (nullable = true)\n |-- hashtags: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- id: string (nullable = true)\n |-- in_reply_to_status_id: string (nullable = true)\n |-- in_reply_to_user_id_str: string (nullable = true)\n |-- location: string (nullable = true)\n |-- reply_count: long (nullable = true)\n |-- retweet_count: long (nullable = true)\n |-- source: string (nullable = true)\n |-- user: struct (nullable = true)\n |    |-- contributors_enabled: boolean (nullable = true)\n |    |-- created_at: string (nullable = true)\n |    |-- default_profile: boolean (nullable = true)\n |    |-- default_profile_image: boolean (nullable = true)\n |    |-- description: string (nullable = true)\n |    |-- favourites_count: long (nullable = true)\n |    |-- follow_request_sent: string (nullable = true)\n |    |-- followers_count: long (nullable = true)\n |    |-- following: string (nullable = true)\n |    |-- friends_count: long (nullable = true)\n |    |-- geo_enabled: boolean (nullable = true)\n |    |-- id: long (nullable = true)\n |    |-- id_str: string (nullable = true)\n |    |-- is_translator: boolean (nullable = true)\n |    |-- lang: string (nullable = true)\n |    |-- listed_count: long (nullable = true)\n |    |-- location: string (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- notifications: string (nullable = true)\n |    |-- profile_background_color: string (nullable = true)\n |    |-- profile_background_image_url: string (nullable = true)\n |    |-- profile_background_image_url_https: string (nullable = true)\n |    |-- profile_background_tile: boolean (nullable = true)\n |    |-- profile_banner_url: string (nullable = true)\n |    |-- profile_image_url: string (nullable = true)\n |    |-- profile_image_url_https: string (nullable = true)\n |    |-- profile_link_color: string (nullable = true)\n |    |-- profile_sidebar_border_color: string (nullable = true)\n |    |-- profile_sidebar_fill_color: string (nullable = true)\n |    |-- profile_text_color: string (nullable = true)\n |    |-- profile_use_background_image: boolean (nullable = true)\n |    |-- protected: boolean (nullable = true)\n |    |-- screen_name: string (nullable = true)\n |    |-- statuses_count: long (nullable = true)\n |    |-- time_zone: string (nullable = true)\n |    |-- translator_type: string (nullable = true)\n |    |-- url: string (nullable = true)\n |    |-- utc_offset: string (nullable = true)\n |    |-- verified: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_twitter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2460671-b6a6-47bd-84ba-e63c00bd541a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### From the user nestec col select the following cols only id_str,followers_count,friends_count and created at \n",
    "# (2 points)\n",
    "from pyspark.sql.functions import col \n",
    "df_twitter=df_twitter.select(col(\"created_at\"),\n",
    "                             col(\"favorite_count\"),\n",
    "                             col(\"hashtags\"),\n",
    "                             col(\"id\"),\n",
    "                             col(\"in_reply_to_status_id\"),\n",
    "                             col(\"in_reply_to_user_id_str\"),\n",
    "                             col(\"location\"),\n",
    "                             col(\"reply_count\"),\n",
    "                             col(\"retweet_count\"),\n",
    "                             col(\"source\"),\n",
    "                             col(\"user.id_str\"),\n",
    "                             col(\"user.followers_count\"),\n",
    "                             col(\"user.friends_count\"),\n",
    "                             col(\"user.created_at\").alias(\"user_created_at\")\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b471703-d40c-4378-b313-09ada92d7916",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- created_at: string (nullable = true)\n |-- favorite_count: long (nullable = true)\n |-- hashtags: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- id: string (nullable = true)\n |-- in_reply_to_status_id: string (nullable = true)\n |-- in_reply_to_user_id_str: string (nullable = true)\n |-- location: string (nullable = true)\n |-- reply_count: long (nullable = true)\n |-- retweet_count: long (nullable = true)\n |-- source: string (nullable = true)\n |-- id_str: string (nullable = true)\n |-- followers_count: long (nullable = true)\n |-- friends_count: long (nullable = true)\n |-- user_created_at: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_twitter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5404794-ac4a-40d0-920d-15415f23f764",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: 15894"
     ]
    }
   ],
   "source": [
    "# Print the total count of number of records in df_twitter(1 point)\n",
    "df_twitter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b937f2f-1ee5-4f6b-82fc-dd26c80c1318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n|   extracted_source|              source|\n+-------------------+--------------------+\n|    Twitter Web App|<a href=\"https://...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|    Twitter Web App|<a href=\"https://...|\n| Twitter Web Client|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n|    Twitter Web App|<a href=\"https://...|\n|Twitter for Android|<a href=\"http://t...|\n|    Twitter Web App|<a href=\"https://...|\n| Twitter for iPhone|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n| Twitter for iPhone|<a href=\"http://t...|\n|Twitter for Android|<a href=\"http://t...|\n+-------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Extract the source lable from source col by droping the anchor tab and save it as another col named extracted_source\n",
    "# for example <a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a> => Twitter Web App\n",
    "# you can use \"<a [^>]+>([^<]+)\" as regualr expresion and the group would be 1 for this regular expression.\n",
    "#(4 points)\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "df_twitter=df_twitter.withColumn(\"extracted_source\", regexp_extract(col(\"source\"), '<a [^>]+>([^<]+)', 1))\n",
    "df_twitter.select(col('extracted_source'),col('source')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc2a2ab-95df-4a32-afab-bc1519176136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame into RDD\n",
    "rdd_twitter=df_twitter.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe54b15-11ee-451e-ad62-188a3537fe85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporay table in memory with name as twitter (1 point)\n",
    "df_twitter.createOrReplaceTempView(\"twitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3bf085-81dd-4900-b4da-d8992988eef9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Analyze Data\n",
    "\n",
    "#### You will be writing code to find the answer to the questions listed below using Just RDD, Using spark SQL \n",
    "\n",
    "- Analyze using RDD \n",
    "- Analyze using Dataframe without temp table \n",
    "- Analyze using spark.sql with temp table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3718e4f-b9bb-47d4-bcdb-1b95060ea597",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Get total number of unique users (1 point for each type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3a7e59-4e9c-4bcd-83c0-95b20af7409f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique users: 14094\n"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "user_index = df_twitter.columns.index('id_str')\n",
    "unique_users_rdd = rdd_twitter.map(lambda row: row[user_index]).distinct()\n",
    "total_unique_users = unique_users_rdd.count()\n",
    "print(f\"Total number of unique users: {total_unique_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8811c4f1-b7e9-402c-b173-d911d3fbd30b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique users: 14094\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "total_unique_users_df = df_twitter.select(col('id_str')).distinct().count()\n",
    "print(f\"Total number of unique users: {total_unique_users_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166afb53-4bf9-4188-8e34-4d6860be63d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|count(DISTINCT id_str)|\n+----------------------+\n|                 14094|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "spark.sql(\"SELECT COUNT(DISTINCT id_str) FROM twitter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "026629d1-5215-40c7-a274-33514f261eef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Get count of user who have more than 1 tweet in the data (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2088c395-da3a-409a-8c94-8fa678d3593c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with more than 1 tweet in the data: 1016\n"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "user_index = df_twitter.columns.index('id_str')\n",
    "tweet_count = rdd_twitter.map(lambda row: row[user_index]).map(lambda user: (user, 1)).reduceByKey(lambda x, y: x + y)\n",
    "count = tweet_count.filter(lambda x: x[1] > 1).count()\n",
    "print('Number of users with more than 1 tweet in the data:', count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71465c1d-d375-4052-adbe-e95dd671b955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with more than 1 tweet in the data: 1016\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "tweet_count = df_twitter.groupby(\"id_str\").count().filter(\"count > 1\").count()\n",
    "print('Number of users with more than 1 tweet in the data:', tweet_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5e04a7-13da-4c8f-b2fc-6af9a3791722",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|    1016|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "spark.sql('SELECT COUNT(*) FROM (SELECT COUNT(id_str) AS tweet_count FROM twitter GROUP BY id_str) WHERE tweet_count > 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6d8f82-ab2b-4bc9-b266-b2486624f2de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Get total number unique extracted_source (1 point each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bcdc745-55f2-4e9c-84d7-267086ba437c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique extracted_source : 133\n"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "extracted_source_index = df_twitter.columns.index('extracted_source')\n",
    "extracted_source_rdd = rdd_twitter.map(lambda row: row[extracted_source_index]).distinct().count()\n",
    "print('Number of unique extracted_source :', extracted_source_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9974fd-f96e-4b8c-97c8-fcbae505dd91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique extracted_source : 133\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "source_count = df_twitter.select(\"extracted_source\").distinct().count()\n",
    "print('Number of unique extracted_source :', source_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f436e2ae-19ba-46ba-825c-18d97c5f30bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n|count(DISTINCT extracted_source)|\n+--------------------------------+\n|                             133|\n+--------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "spark.sql('SELECT COUNT(DISTINCT extracted_source) FROM twitter').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188bdf59-cf92-4d08-927b-f003e0098ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Get top 5 most used extracted_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07037b6-c226-4de2-b942-da6f732b20ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: [('Twitter for Android', 6262),\n ('Twitter for iPhone', 5698),\n ('Twitter Web App', 2878),\n ('Twitter for iPad', 428),\n ('Twitter Web Client', 136)]"
     ]
    }
   ],
   "source": [
    "# Using RDD (5 points)\n",
    "extracted_source_top = (\n",
    "    rdd_twitter\n",
    "    .map(lambda row: (row[extracted_source_index], 1)) \n",
    "    .reduceByKey(lambda x, y: x + y)  \n",
    "    .sortBy(lambda x: x[1], ascending=False) \n",
    ")\n",
    "extracted_source_top.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8175f756-d354-46b9-8ecb-24798c606293",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|   extracted_source|count|\n+-------------------+-----+\n|Twitter for Android| 6262|\n| Twitter for iPhone| 5698|\n|    Twitter Web App| 2878|\n|   Twitter for iPad|  428|\n| Twitter Web Client|  136|\n+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame (2 points)\n",
    "df_twitter.groupby(\"extracted_source\").count().sort(\"count\", ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff8fab4-c6b3-4dec-ba2e-525ef27eb304",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|   extracted_source|count|\n+-------------------+-----+\n|Twitter for Android| 6262|\n| Twitter for iPhone| 5698|\n|    Twitter Web App| 2878|\n|   Twitter for iPad|  428|\n| Twitter Web Client|  136|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table. (2 points)\n",
    "spark.sql('SELECT extracted_source, COUNT(extracted_source) as count FROM twitter GROUP BY extracted_source ORDER BY count DESC LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b302336-03bd-448f-9608-a63b4ef1daa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.5 Get count of distinct hastags used ( 5 point each) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945573b7-aaa7-44d0-8086-4230a8f81d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: 1215"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "hashtags_index = df_twitter.columns.index('hashtags')\n",
    "hashtags_count = rdd_twitter.flatMap(lambda row: row[hashtags_index])\n",
    "hashtags_count.filter(lambda hashtags: hashtags is not None and len(hashtags) > 0).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34af0927-8b11-4c2c-b653-aabc3b5de016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct hastags used is: 1215\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "hashtags_count  = df_twitter.select(explode(col('hashtags'))).distinct().count()\n",
    "print('The number of distinct hastags used is:', hashtags_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de8afaa-e618-46b7-a471-786267e8da6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|    1215|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "spark.sql('SELECT COUNT(*) FROM (SELECT DISTINCT EXPLODE(hashtags) FROM twitter)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cedd1db9-f6e1-4114-8187-76e447dd568b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.6 Get top 5 hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae2407d-566e-4071-b2be-b85930690339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: [('طبق_القدرات_للثانويه_ياريس', 385),\n ('Corona', 319),\n ('OilPrice', 251),\n ('COVID19', 125),\n ('corona', 123)]"
     ]
    }
   ],
   "source": [
    "# Using RDD (4 points)\n",
    "hashtags_index = df_twitter.columns.index('hashtags')\n",
    "hashtags_top = rdd_twitter.flatMap(lambda row: row[hashtags_index])\n",
    "hashtags_top.map(lambda hashtags: (hashtags, 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739048d3-575b-49ed-a5bc-f92cd30ba224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|                 col|count|\n+--------------------+-----+\n|طبق_القدرات_للثان...|  385|\n|              Corona|  319|\n|            OilPrice|  251|\n|             COVID19|  125|\n|              corona|  123|\n+--------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame (2 points)\n",
    "df_twitter.select(explode(col('hashtags'))).groupby(\"col\").count().sort(\"count\", ascending = False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8357166-5055-4127-a832-1df5ac844808",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|                 col|count|\n+--------------------+-----+\n|طبق_القدرات_للثان...|  385|\n|              Corona|  319|\n|            OilPrice|  251|\n|             COVID19|  125|\n|              corona|  123|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table. (2 points)\n",
    "spark.sql('SELECT col, COUNT(col) as count FROM (SELECT EXPLODE(hashtags) FROM twitter) GROUP BY col ORDER BY count DESC LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2e235a-34d7-4c48-947b-c092387a21ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.6 Get total number of tweets which are retweeted more than 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ccc01a7-f5ea-48bd-9bac-8b5dfe7fba0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: 15753"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "retweet_count_index = df_twitter.columns.index('retweet_count')\n",
    "rdd_twitter.filter(lambda row: row[retweet_count_index] > 100).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9502388c-f59a-4dc1-82a1-255cab5af332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets which are retweeted more than 100 times is: 15753\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "count_100 = df_twitter.filter(\"retweet_count > 100\").count()\n",
    "print('The number of tweets which are retweeted more than 100 times is:', count_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be34af63-a4e5-40ae-8f2e-dae437a0f1b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|count(id)|\n+---------+\n|    15753|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "spark.sql('SELECT COUNT(id) FROM twitter WHERE retweet_count > 100').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48189fee-024e-4ff5-8109-da51120e3453",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.8 Get top 3 most retweeted tweets per country (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b894a602-d6ee-44b5-9c4c-96c92ccf8309",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: [('India',\n  [('1252332114948874240', 9988),\n   ('1252252336921206787', 9976),\n   ('1252254519116746754', 9973)]),\n ('Pakistan',\n  [('1252334264248606720', 9988),\n   ('1252251912084357121', 9975),\n   ('1252252126694309888', 9973)]),\n ('USA',\n  [('1252331777806524416', 9994),\n   ('1252254239805579264', 9987),\n   ('1252335464750735362', 9982)]),\n ('Italy',\n  [('1252252106750377996', 9994),\n   ('1252251206027816960', 9984),\n   ('1252330500670337024', 9971)]),\n ('Canada',\n  [('1252335430323888128', 9997),\n   ('1252254877939531776', 9992),\n   ('1252252082825986051', 9987)]),\n ('China',\n  [('1252335780707684352', 9998),\n   ('1252253596516843520', 9993),\n   ('1252255562525560832', 9984)]),\n ('Chile',\n  [('1252253612140490759', 9988),\n   ('1252334891951427585', 9984),\n   ('1252253710182481920', 9978)]),\n ('UK',\n  [('1252333018578145280', 9991),\n   ('1252252091822870529', 9989),\n   ('1252254043973603329', 9985)]),\n ('Mexico',\n  [('1252253843145912320', 9998),\n   ('1252255209776189442', 9994),\n   ('1252252016006422533', 9971)]),\n ('Spain',\n  [('1252335445876367361', 9992),\n   ('1252334839094599681', 9981),\n   ('1252254696112300032', 9969)]),\n ('Germany',\n  [('1252334028092399622', 9999),\n   ('1252330902325248000', 9997),\n   ('1252252295510855682', 9990)])]"
     ]
    }
   ],
   "source": [
    "# Using RDD\n",
    "location_index = df_twitter.columns.index('location')\n",
    "id_index = df_twitter.columns.index('id')\n",
    "retweet_count_index = df_twitter.columns.index('retweet_count')\n",
    "location_rdd = rdd_twitter.map(lambda row: (row[location_index], (row[id_index], row[retweet_count_index]))).groupByKey().mapValues(lambda tweet: sorted(tweet, key=lambda x: x[1], reverse=True)[:3])\n",
    "location_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08dd149d-2c03-459f-b281-4488db7241bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------------+----+\n|                 id|location|retweet_count|rank|\n+-------------------+--------+-------------+----+\n|1252335430323888128|  Canada|         9997|   1|\n|1252254877939531776|  Canada|         9992|   2|\n|1252252082825986051|  Canada|         9987|   3|\n|1252253612140490759|   Chile|         9988|   1|\n|1252334891951427585|   Chile|         9984|   2|\n|1252253710182481920|   Chile|         9978|   3|\n|1252335780707684352|   China|         9998|   1|\n|1252253596516843520|   China|         9993|   2|\n|1252255562525560832|   China|         9984|   3|\n|1252334028092399622| Germany|         9999|   1|\n|1252330902325248000| Germany|         9997|   2|\n|1252252295510855682| Germany|         9990|   3|\n|1252332114948874240|   India|         9988|   1|\n|1252252336921206787|   India|         9976|   2|\n|1252254519116746754|   India|         9973|   3|\n|1252252106750377996|   Italy|         9994|   1|\n|1252251206027816960|   Italy|         9984|   2|\n|1252330500670337024|   Italy|         9971|   3|\n|1252253843145912320|  Mexico|         9998|   1|\n|1252255209776189442|  Mexico|         9994|   2|\n+-------------------+--------+-------------+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "df_twitter.select('id','location','retweet_count', rank().over(Window().partitionBy(\"location\").orderBy(col(\"retweet_count\").desc())).alias('rank')).sort(\"retweet_count\", ascending=False).filter('rank <= 3').sort(\"location\", \"rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f26dc93-f30e-46c5-b959-dc284fc0dbc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------+\n|location|                 id|retweet_count|\n+--------+-------------------+-------------+\n|  Canada|1252335430323888128|         9997|\n|  Canada|1252254877939531776|         9992|\n|  Canada|1252252082825986051|         9987|\n|   Chile|1252253612140490759|         9988|\n|   Chile|1252334891951427585|         9984|\n|   Chile|1252253710182481920|         9978|\n|   China|1252335780707684352|         9998|\n|   China|1252253596516843520|         9993|\n|   China|1252255562525560832|         9984|\n| Germany|1252334028092399622|         9999|\n| Germany|1252330902325248000|         9997|\n| Germany|1252252295510855682|         9990|\n|   India|1252332114948874240|         9988|\n|   India|1252252336921206787|         9976|\n|   India|1252254519116746754|         9973|\n|   Italy|1252252106750377996|         9994|\n|   Italy|1252251206027816960|         9984|\n|   Italy|1252330500670337024|         9971|\n|  Mexico|1252253843145912320|         9998|\n|  Mexico|1252255209776189442|         9994|\n+--------+-------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table.\n",
    "spark.sql('SELECT location, id, retweet_count FROM (SELECT *, RANK() OVER (PARTITION BY location ORDER BY retweet_count DESC) as rank FROM twitter) WHERE rank <= 3 ORDER BY location, rank ASC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29ffc84-4cf0-4588-90dc-84fc373c1855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.9 Total number of tweets per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74d26ec-8348-40e2-948d-5b89ae4df1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[33]: [('India', 1480),\n ('Pakistan', 1470),\n ('USA', 1539),\n ('Italy', 1422),\n ('Canada', 1441),\n ('China', 1457),\n ('Chile', 1410),\n ('UK', 1376),\n ('Mexico', 1409),\n ('Spain', 1464),\n ('Germany', 1426)]"
     ]
    }
   ],
   "source": [
    "# Using RDD (3 points)\n",
    "location_index = df_twitter.columns.index('location')\n",
    "rdd_twitter.map(lambda row: row[location_index]).map(lambda location: (location, 1)).reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672a3e28-433e-4669-9612-2931ee30563c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n|location|count|\n+--------+-----+\n| Germany| 1426|\n|   India| 1480|\n|   China| 1457|\n|   Chile| 1410|\n|   Italy| 1422|\n|   Spain| 1464|\n|     USA| 1539|\n|  Mexico| 1409|\n|      UK| 1376|\n|  Canada| 1441|\n|Pakistan| 1470|\n+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame (2 points)\n",
    "df_twitter.groupBy(\"location\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24294129-3f68-4d94-b6e1-627f0f23973d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n|location|count(id)|\n+--------+---------+\n| Germany|     1426|\n|   India|     1480|\n|   China|     1457|\n|   Chile|     1410|\n|   Italy|     1422|\n|   Spain|     1464|\n|     USA|     1539|\n|  Mexico|     1409|\n|      UK|     1376|\n|  Canada|     1441|\n|Pakistan|     1470|\n+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Using spark.sql and the temporay table. (1 point)\n",
    "spark.sql('SELECT location, COUNT(id) FROM twitter GROUP BY location').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8907e096-9205-47b6-b76a-513f08a15b7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a90e738-eea4-47d2-b029-629ac1d14f50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.1 save the data such that you have seperate folder per country (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3413113-fbdf-477a-8473-5dad5cc94a02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4093460093223405>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Using \u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m df_twitter \u001B[38;5;241m=\u001B[39m df_twitter\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhashtags\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhashtags\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[0;32m----> 3\u001B[0m df_twitter\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, header \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   1797\u001B[0m )\n",
       "\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/location already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4093460093223405>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Using \u001B[39;00m\n\u001B[1;32m      2\u001B[0m df_twitter \u001B[38;5;241m=\u001B[39m df_twitter\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhashtags\u001B[39m\u001B[38;5;124m\"\u001B[39m, col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhashtags\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m----> 3\u001B[0m df_twitter\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, header \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1797\u001B[0m )\n\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/location already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/location already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using \n",
    "df_twitter = df_twitter.withColumn(\"hashtags\", col(\"hashtags\").cast(\"string\"))\n",
    "df_twitter.write.partitionBy('location').csv(\"location\", header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ebbb99a-7886-45a1-a992-8259f7a0c8e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+-------------------+---------------------+-----------------------+-----------+-------------+--------------------+-------------------+---------------+-------------+--------------------+-------------------+--------+\n|          created_at|favorite_count|            hashtags|                 id|in_reply_to_status_id|in_reply_to_user_id_str|reply_count|retweet_count|              source|             id_str|followers_count|friends_count|     user_created_at|   extracted_source|location|\n+--------------------+--------------+--------------------+-------------------+---------------------+-----------------------+-----------+-------------+--------------------+-------------------+---------------+-------------+--------------------+-------------------+--------+\n|Mon Apr 20 15:14:...|          9038|          [COVID_19]|1252254259728355329|                 null|                   null|       6063|         7732|<a href=\"http://t...| 934058010875330561|            201|           46|Fri Nov 24 13:55:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          7193|                  []|1252254262798823429|                 null|                   null|       6249|         2216|<a href=\"http://i...|1185081450183512065|              1|            5|Fri Oct 18 06:33:...|          Instagram|     USA|\n|Mon Apr 20 15:14:...|          6910|                  []|1252254263805435910|                 null|                   null|       4382|         1777|<a href=\"http://t...|         3229308795|           3273|         3836|Sat May 02 23:33:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          8026|[طبق_القدرات_للثا...|1252254264308760584|                 null|                   null|       2705|         6083|<a href=\"https://...|1251817831232090112|              6|          122|Sun Apr 19 10:20:...|    Twitter Web App|     USA|\n|Mon Apr 20 15:14:...|          4664|                  []|1252254274836283393|                 null|                   null|       7554|         7288|<a href=\"http://t...|           20071488|           6459|         6556|Wed Feb 04 17:27:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          4894|            [Corona]|1252254284457992192|                 null|                   null|       3724|         6452|<a href=\"http://t...|1165197561059110912|             13|           19|Sat Aug 24 09:42:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          2503|                  []|1252254286144290819|                 null|                   null|       6410|         3577|<a href=\"http://t...|         3365340173|           1273|         2038|Wed Jul 08 03:49:...|   Twitter for iPad|     USA|\n|Mon Apr 20 15:14:...|          2169|                  []|1252254288421834753|                 null|                   null|        950|         3554|<a href=\"http://t...|1226912794311827456|           1249|          720|Mon Feb 10 16:56:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          9237|                  []|1252254291261304834|                 null|                   null|       8181|         2245|<a href=\"http://t...|         2792747759|             27|          190|Mon Sep 29 20:17:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          3196|[Resistance, NotM...|1252254295199830019|  1252048195993116674|     821865849476829184|       9075|         3523|<a href=\"http://t...|         2357366803|           3244|         4988|Sun Feb 23 03:31:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          7870|                  []|1252254307681853442|                 null|                   null|       6264|         6827|<a href=\"http://t...|1237017416821022720|            181|          679|Mon Mar 09 14:08:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          2618|                  []|1252254307526860803|                 null|                   null|       6033|         8200|<a href=\"http://t...|1148953452065640449|             15|          875|Wed Jul 10 13:53:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          7987|                  []|1252254309015646208|                 null|                   null|       2457|         9321|<a href=\"http://t...|          528329835|            288|         1427|Sun Mar 18 09:17:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          4180|                  []|1252254324161462273|  1252166414380814336|              476929955|       7131|         3871|<a href=\"http://t...|         2235175580|           3242|         3113|Sat Dec 07 23:13:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          5365|                  []|1252254329781837830|  1251906307747205121|     989885349601005568|       6884|         6448|<a href=\"http://t...|          275172011|            348|         1128|Thu Mar 31 19:29:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          7555|                  []|1252254342888882176|                 null|                   null|       3799|         1952|<a href=\"http://t...|1135347284739207169|            294|          433|Mon Jun 03 00:47:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          6146|                  []|1252254343484579844|                 null|                   null|       9857|         8988|<a href=\"http://t...|          763500709|            424|          435|Fri Aug 17 11:01:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          9504|                  []|1252254348077412352|                 null|                   null|       1184|         6551|<a href=\"http://t...|1027493738791292928|            615|          360|Thu Aug 09 09:56:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          9140|                  []|1252254350774358018|                 null|                   null|         21|         3002|<a href=\"http://t...|         1574424822|            845|          426|Sun Jul 07 06:14:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          1222|                  []|1252254355043913729|                 null|                   null|       7584|         9524|<a href=\"http://t...|1198591156889604099|            130|           44|Sun Nov 24 13:16:...|Twitter for Android|     USA|\n+--------------------+--------------+--------------------+-------------------+---------------------+-----------------------+-----------+-------------+--------------------+-------------------+---------------+-------------+--------------------+-------------------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"dbfs:/location\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f27209-6a78-4c31-b50a-cfc91ceea32b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.2 Save the data as parquet files (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e65d90-76a0-4fd8-82ff-58f7258fd49e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using DataFrame\n",
    "df_twitter.write.partitionBy('location').parquet(\"parquet_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bec053be-35be-4f3b-a15a-6d900be93074",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+-------------------+---------------------+-----------------------+-----------+-------------+--------------------+-------------------+---------------+-------------+--------------------+-------------------+--------+\n|          created_at|favorite_count|            hashtags|                 id|in_reply_to_status_id|in_reply_to_user_id_str|reply_count|retweet_count|              source|             id_str|followers_count|friends_count|     user_created_at|   extracted_source|location|\n+--------------------+--------------+--------------------+-------------------+---------------------+-----------------------+-----------+-------------+--------------------+-------------------+---------------+-------------+--------------------+-------------------+--------+\n|Mon Apr 20 15:14:...|          9038|          [COVID_19]|1252254259728355329|                 null|                   null|       6063|         7732|<a href=\"http://t...| 934058010875330561|            201|           46|Fri Nov 24 13:55:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          7193|                  []|1252254262798823429|                 null|                   null|       6249|         2216|<a href=\"http://i...|1185081450183512065|              1|            5|Fri Oct 18 06:33:...|          Instagram|     USA|\n|Mon Apr 20 15:14:...|          6910|                  []|1252254263805435910|                 null|                   null|       4382|         1777|<a href=\"http://t...|         3229308795|           3273|         3836|Sat May 02 23:33:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          8026|[طبق_القدرات_للثا...|1252254264308760584|                 null|                   null|       2705|         6083|<a href=\"https://...|1251817831232090112|              6|          122|Sun Apr 19 10:20:...|    Twitter Web App|     USA|\n|Mon Apr 20 15:14:...|          4664|                  []|1252254274836283393|                 null|                   null|       7554|         7288|<a href=\"http://t...|           20071488|           6459|         6556|Wed Feb 04 17:27:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          4894|            [Corona]|1252254284457992192|                 null|                   null|       3724|         6452|<a href=\"http://t...|1165197561059110912|             13|           19|Sat Aug 24 09:42:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          2503|                  []|1252254286144290819|                 null|                   null|       6410|         3577|<a href=\"http://t...|         3365340173|           1273|         2038|Wed Jul 08 03:49:...|   Twitter for iPad|     USA|\n|Mon Apr 20 15:14:...|          2169|                  []|1252254288421834753|                 null|                   null|        950|         3554|<a href=\"http://t...|1226912794311827456|           1249|          720|Mon Feb 10 16:56:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          9237|                  []|1252254291261304834|                 null|                   null|       8181|         2245|<a href=\"http://t...|         2792747759|             27|          190|Mon Sep 29 20:17:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          3196|[Resistance, NotM...|1252254295199830019|  1252048195993116674|     821865849476829184|       9075|         3523|<a href=\"http://t...|         2357366803|           3244|         4988|Sun Feb 23 03:31:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          7870|                  []|1252254307681853442|                 null|                   null|       6264|         6827|<a href=\"http://t...|1237017416821022720|            181|          679|Mon Mar 09 14:08:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          2618|                  []|1252254307526860803|                 null|                   null|       6033|         8200|<a href=\"http://t...|1148953452065640449|             15|          875|Wed Jul 10 13:53:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          7987|                  []|1252254309015646208|                 null|                   null|       2457|         9321|<a href=\"http://t...|          528329835|            288|         1427|Sun Mar 18 09:17:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          4180|                  []|1252254324161462273|  1252166414380814336|              476929955|       7131|         3871|<a href=\"http://t...|         2235175580|           3242|         3113|Sat Dec 07 23:13:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          5365|                  []|1252254329781837830|  1251906307747205121|     989885349601005568|       6884|         6448|<a href=\"http://t...|          275172011|            348|         1128|Thu Mar 31 19:29:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          7555|                  []|1252254342888882176|                 null|                   null|       3799|         1952|<a href=\"http://t...|1135347284739207169|            294|          433|Mon Jun 03 00:47:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          6146|                  []|1252254343484579844|                 null|                   null|       9857|         8988|<a href=\"http://t...|          763500709|            424|          435|Fri Aug 17 11:01:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          9504|                  []|1252254348077412352|                 null|                   null|       1184|         6551|<a href=\"http://t...|1027493738791292928|            615|          360|Thu Aug 09 09:56:...|Twitter for Android|     USA|\n|Mon Apr 20 15:14:...|          9140|                  []|1252254350774358018|                 null|                   null|         21|         3002|<a href=\"http://t...|         1574424822|            845|          426|Sun Jul 07 06:14:...| Twitter for iPhone|     USA|\n|Mon Apr 20 15:14:...|          1222|                  []|1252254355043913729|                 null|                   null|       7584|         9524|<a href=\"http://t...|1198591156889604099|            130|           44|Sun Nov 24 13:16:...|Twitter for Android|     USA|\n+--------------------+--------------+--------------------+-------------------+---------------------+-----------------------+-----------+-------------+--------------------+-------------------+---------------+-------------+--------------------+-------------------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(\"dbfs:/parquet_location\", header=True)\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Assignment_2_Jaume_Sanchez (1)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
